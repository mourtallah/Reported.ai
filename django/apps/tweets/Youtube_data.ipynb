{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d593f9",
   "metadata": {},
   "source": [
    "Youtube title extraction \n",
    "\n",
    "\n",
    "In this file trying to fetch youtube data and filtering out all unncecessary parameter from the youtube.\n",
    "importing the important libraray, you data  is used to get data from Youtube Data API\n",
    "this recommender system will be able to recommend videos to us, based on top news topics and\n",
    "based on combination of features.\n",
    "First, we load the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c893cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import re \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib.pyplot import xlabel,ylabel,title\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tweepy\n",
    "from tweepy import API\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c5491",
   "metadata": {},
   "source": [
    "1. Gather the data\n",
    " import data from Youtube Data API\n",
    "authentication details to connect with twittet api to fetch data.\n",
    "For this problem I decided to retrieve the dataset based on top twitter topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f20d1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672\n",
      "67098\n",
      "youtube#video\n",
      "Europe struggles with major wildfires and energy uncertainty amid 'heat apocalypse'\n",
      "2760\n",
      "310638\n",
      "youtube#video\n",
      "Massive California wildfire erupts\n",
      "290\n",
      "31607\n",
      "youtube#video\n",
      "Record heat in the Northwest as wildfires continue l GMA\n",
      "53\n",
      "4047\n",
      "youtube#video\n",
      "Wildfire In Texas Scorches 500 Acres As Crews Continue To Battle The Blaze\n",
      "44\n",
      "8302\n",
      "youtube#video\n",
      "Raw: New wildfire near Colfax prompts evacuation order\n"
     ]
    }
   ],
   "source": [
    "title = []\n",
    "viewcount = []\n",
    "likecount = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "youtube = build('youtube', 'v3', developerKey='AIzaSyBfoGn0960ZupAD7YiIdwfRe1MDbdg9F_U')\n",
    "\n",
    "    \n",
    "video_statistics= youtube.videos().list(id =['WluvF8Tj5tc','chZp2U09Qa8','frJV7qx9v6Y','KsnjW-sLNTo','FUQXSz4yV-w'],\n",
    "                                        part=['statistics','snippet']).execute()\n",
    "#print(video_statistics)\n",
    "\n",
    "for i in range(5):\n",
    "    likecount = int(video_statistics['items'][i]['statistics']['likeCount'])\n",
    "    viewcount = int(video_statistics['items'][i]['statistics']['viewCount'])\n",
    "    video_kind = video_statistics['items'][i]['kind']\n",
    "   \n",
    "    title = video_statistics['items'][i]['snippet']['title']\n",
    "    print(likecount)\n",
    "    print(viewcount)\n",
    "    print(video_kind)\n",
    "\n",
    "    print(title)\n",
    "  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8025ae23",
   "metadata": {},
   "source": [
    "first content-based recommender will have a goal of recommending videos which have a similar plot to a selected tweet topics.\n",
    "searching th youtube title based on particular video id and extracting the youtube  data, like count, viewcount,title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67553c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672\n",
      "67098\n",
      "youtube#video\n",
      "Europe struggles with major wildfires and energy uncertainty amid 'heat apocalypse'\n",
      "2760\n",
      "310638\n",
      "youtube#video\n",
      "Massive California wildfire erupts\n",
      "290\n",
      "31607\n",
      "youtube#video\n",
      "Record heat in the Northwest as wildfires continue l GMA\n",
      "17542\n",
      "481948\n",
      "youtube#video\n",
      "Pavlich: Democrats are living by their own rules now on illegal immigration\n",
      "53\n",
      "4047\n",
      "youtube#video\n",
      "Wildfire In Texas Scorches 500 Acres As Crews Continue To Battle The Blaze\n",
      "44\n",
      "8302\n",
      "youtube#video\n",
      "Raw: New wildfire near Colfax prompts evacuation order\n",
      "22654\n",
      "2881207\n",
      "youtube#video\n",
      "Growth, Cities, and Immigration: Crash Course US History #25\n",
      "356\n",
      "56642\n",
      "youtube#video\n",
      "Kayaker found dead at Lake Mead after severe weather\n",
      "294\n",
      "40347\n",
      "youtube#video\n",
      "Do monsoons impact Lake Mead's water level?\n",
      "71\n",
      "8424\n",
      "youtube#video\n",
      "U.S. stocks tumble over fears of another Fed rate hike\n",
      "7286\n",
      "531358\n",
      "youtube#video\n",
      "How The U.S. Made Inflation Worse\n",
      "33\n",
      "849\n",
      "youtube#video\n",
      "Energy bills, food costs rising with inflation\n",
      "80\n",
      "8180\n",
      "youtube#video\n",
      "Senior adviser to Biden says Inflation Reduction Act will reduce inflation l ABCNL\n",
      "1650\n",
      "76181\n",
      "youtube#video\n",
      "Can artificial intelligence become sentient, or smarter than we are - and then what? | Techtopia\n",
      "298048\n",
      "11705868\n",
      "youtube#video\n",
      "But what is a neural network? | Chapter 1, Deep learning\n",
      "1390\n",
      "73020\n",
      "youtube#video\n",
      "2021's Biggest Advancements in Artificial Intelligence\n",
      "84\n",
      "9101\n",
      "youtube#video\n",
      "The financial markets have responded appropriately to Fed moves, says Schroder's Insana\n",
      "574\n",
      "48730\n",
      "youtube#video\n",
      "Jim Cramer breaks down Thursday's market action and what it means for investors\n",
      "32\n",
      "3120\n",
      "youtube#video\n",
      "U.S. prepares to roll out new Covid vaccines this fall\n",
      "1173903\n",
      "338939585\n",
      "youtube#video\n",
      "COVID-19 Animation: What Happens If You Get Coronavirus?\n"
     ]
    }
   ],
   "source": [
    "title1 = []\n",
    "viewcount1 = []\n",
    "likecount1 = []\n",
    "try:\n",
    "    os.remove('tweets1data.csv')\n",
    "    #print \"File deleted\"\n",
    "except:\n",
    "    #print \"File was not present already\"\n",
    "    pass\n",
    "\n",
    "csvFile = open('youtubedata.csv', 'w')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "youtube = build('youtube', 'v3', developerKey='AIzaSyBfoGn0960ZupAD7YiIdwfRe1MDbdg9F_U')\n",
    "\n",
    "    \n",
    "video_statistics= youtube.videos().list(id =['WluvF8Tj5tc','chZp2U09Qa8','frJV7qx9v6Y',\n",
    "                                             \"Rtjc3HIvFkY\",'KsnjW-sLNTo','FUQXSz4yV-w',\n",
    "                                             'RRhjqqe750A','Xkyq03pfYUs','Z5p0hhYmDFg',\n",
    "                                             'Zn7WhBZ8J-M','f60Z4epksOk','erinPN6glcE',\n",
    "                                             'I82qvaRmybM','lcUk1cYWY9I','aircAruvnKk',\n",
    "                                             't4B99T_3IsM','6Gmohrod2kY','2pBRZ29yWyY','Bgv1KInL2ek','5DGwOJXSxqg'],\n",
    "                                        part=['statistics','snippet',]).execute()\n",
    "\n",
    "\n",
    "\n",
    "#print(video_statistics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    likecount = int(video_statistics['items'][i]['statistics']['likeCount'])\n",
    "    viewcount = int(video_statistics['items'][i]['statistics']['viewCount'])\n",
    "    video_kind = video_statistics['items'][i]['kind']\n",
    "   \n",
    "    title = video_statistics['items'][i]['snippet']['title']\n",
    "    youtube_data = pd.DataFrame({'Title':title1,'View_Count':viewcount1,'Like_Count':likecount1})\n",
    "    youtube_data[title] = viewcount\n",
    "    print(likecount)\n",
    "    print(viewcount)\n",
    "    print(video_kind)\n",
    "    title1.append(title)\n",
    "    likecount1.append(likecount)\n",
    "    viewcount1.append(viewcount)\n",
    "\n",
    "    print(title)\n",
    "  \n",
    "\n",
    "    youtube_data = pd.DataFrame({'Title':title1,'View_Count':viewcount1,'Like_Count':likecount1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9999d42",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a389e",
   "metadata": {},
   "source": [
    "creating data frames for youtube data using pandas with title, view_count,Like_count as header. printing all fetched youtube data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65e8041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title  View_Count  Like_Count\n",
      "0   Europe struggles with major wildfires and ener...       67098         672\n",
      "1                  Massive California wildfire erupts      310638        2760\n",
      "2   Record heat in the Northwest as wildfires cont...       31607         290\n",
      "3   Pavlich: Democrats are living by their own rul...      481948       17542\n",
      "4   Wildfire In Texas Scorches 500 Acres As Crews ...        4047          53\n",
      "5   Raw: New wildfire near Colfax prompts evacuati...        8302          44\n",
      "6   Growth, Cities, and Immigration: Crash Course ...     2881207       22654\n",
      "7   Kayaker found dead at Lake Mead after severe w...       56642         356\n",
      "8         Do monsoons impact Lake Mead's water level?       40347         294\n",
      "9   U.S. stocks tumble over fears of another Fed r...        8424          71\n",
      "10                  How The U.S. Made Inflation Worse      531358        7286\n",
      "11     Energy bills, food costs rising with inflation         849          33\n",
      "12  Senior adviser to Biden says Inflation Reducti...        8180          80\n",
      "13  Can artificial intelligence become sentient, o...       76181        1650\n",
      "14  But what is a neural network? | Chapter 1, Dee...    11705868      298048\n",
      "15  2021's Biggest Advancements in Artificial Inte...       73020        1390\n",
      "16  The financial markets have responded appropria...        9101          84\n",
      "17  Jim Cramer breaks down Thursday's market actio...       48730         574\n",
      "18  U.S. prepares to roll out new Covid vaccines t...        3120          32\n",
      "19  COVID-19 Animation: What Happens If You Get Co...   338939585     1173903\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0cc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = pd.DataFrame(data = youtube_data, columns = ['title','viewcount'],index=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d901dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  View_Count  Like_Count\n",
      "0  Europe struggles with major wildfires and ener...       67098         672\n",
      "1                 Massive California wildfire erupts      310638        2760\n",
      "2  Record heat in the Northwest as wildfires cont...       31607         290\n",
      "3  Pavlich: Democrats are living by their own rul...      481948       17542\n",
      "4  Wildfire In Texas Scorches 500 Acres As Crews ...        4047          53\n"
     ]
    }
   ],
   "source": [
    "print(youtube_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b83931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>View_Count</th>\n",
       "      <th>Like_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe struggles with major wildfires and ener...</td>\n",
       "      <td>67098</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Massive California wildfire erupts</td>\n",
       "      <td>310638</td>\n",
       "      <td>2760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Record heat in the Northwest as wildfires cont...</td>\n",
       "      <td>31607</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pavlich: Democrats are living by their own rul...</td>\n",
       "      <td>481948</td>\n",
       "      <td>17542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wildfire In Texas Scorches 500 Acres As Crews ...</td>\n",
       "      <td>4047</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  View_Count  Like_Count\n",
       "0  Europe struggles with major wildfires and ener...       67098         672\n",
       "1                 Massive California wildfire erupts      310638        2760\n",
       "2  Record heat in the Northwest as wildfires cont...       31607         290\n",
       "3  Pavlich: Democrats are living by their own rul...      481948       17542\n",
       "4  Wildfire In Texas Scorches 500 Acres As Crews ...        4047          53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f525c",
   "metadata": {},
   "source": [
    "this function removes all @ symbolic sentencs in title of th eyoutube data,removes the tags,remove capital letter which are not necessary. filtering out tweets will \n",
    "help to decrease the input data. increase the proccessing speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "170508da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>View_Count</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe struggles with major wildfires and ener...</td>\n",
       "      <td>67098</td>\n",
       "      <td>672</td>\n",
       "      <td>Europe struggles with major wildfires and ener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Massive California wildfire erupts</td>\n",
       "      <td>310638</td>\n",
       "      <td>2760</td>\n",
       "      <td>Massive California wildfire erupts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Record heat in the Northwest as wildfires cont...</td>\n",
       "      <td>31607</td>\n",
       "      <td>290</td>\n",
       "      <td>Record heat in the Northwest as wildfires cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pavlich: Democrats are living by their own rul...</td>\n",
       "      <td>481948</td>\n",
       "      <td>17542</td>\n",
       "      <td>Pavlich: Democrats are living by their own rul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wildfire In Texas Scorches 500 Acres As Crews ...</td>\n",
       "      <td>4047</td>\n",
       "      <td>53</td>\n",
       "      <td>Wildfire In Texas Scorches 500 Acres As Crews ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  View_Count  Like_Count  \\\n",
       "0  Europe struggles with major wildfires and ener...       67098         672   \n",
       "1                 Massive California wildfire erupts      310638        2760   \n",
       "2  Record heat in the Northwest as wildfires cont...       31607         290   \n",
       "3  Pavlich: Democrats are living by their own rul...      481948       17542   \n",
       "4  Wildfire In Texas Scorches 500 Acres As Crews ...        4047          53   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Europe struggles with major wildfires and ener...  \n",
       "1                 Massive California wildfire erupts  \n",
       "2  Record heat in the Northwest as wildfires cont...  \n",
       "3  Pavlich: Democrats are living by their own rul...  \n",
       "4  Wildfire In Texas Scorches 500 Acres As Crews ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "  text = re.sub(r'@[A-Za-z0-9]+','',text) #removes @mentions\n",
    "  text = re.sub(r'#','',text) #removes # tags\n",
    "  text = re.sub(r'\\\\n','',text) #removes # tags\n",
    "  text = re.sub(r'RT[\\s]+','',text) #removes RT\n",
    "  text = re.sub(r'https?:\\/\\/\\S+','',text) #removes urls\n",
    "\n",
    "  return text\n",
    "youtube_data['clean_text']=youtube_data['Title'].apply(clean_text)\n",
    "\n",
    "#display clean text\n",
    "youtube_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e600ce6",
   "metadata": {},
   "source": [
    "TF-IDF approach\n",
    "A statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "TF-IDF of a word in a document which is part of a larger corpus of documents is a combination of two values. One is term frequency (TF), which measures how frequently the word occurs in the document some of the words, such as “the” and “is”, occur frequently in all documents and we want to downscale the importance of such words. This is accomplished by multiplying TF with the inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4932e609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(stop_words='english')\n",
      "  (0, 12)\t0.3467292216518096\n",
      "  (0, 48)\t0.30478029306172993\n",
      "  (0, 10)\t0.3467292216518096\n",
      "  (0, 103)\t0.3467292216518096\n",
      "  (0, 36)\t0.30478029306172993\n",
      "  (0, 108)\t0.30478029306172993\n",
      "  (0, 64)\t0.3467292216518096\n",
      "  (0, 98)\t0.3467292216518096\n",
      "  (0, 38)\t0.3467292216518096\n",
      "  (1, 37)\t0.5249270021222106\n",
      "  (1, 107)\t0.4163591326354764\n",
      "  (1, 21)\t0.5249270021222106\n",
      "  (1, 67)\t0.5249270021222106\n",
      "  (2, 45)\t0.43363635107380566\n",
      "  (2, 25)\t0.3811729900723928\n",
      "  (2, 76)\t0.43363635107380566\n",
      "  (2, 83)\t0.43363635107380566\n",
      "  (2, 48)\t0.3811729900723928\n",
      "  (2, 108)\t0.3811729900723928\n",
      "  (3, 52)\t0.3658542888871263\n",
      "  (3, 51)\t0.4162092356742928\n",
      "  (3, 89)\t0.4162092356742928\n",
      "  (3, 63)\t0.4162092356742928\n",
      "  (3, 35)\t0.4162092356742928\n",
      "  (3, 78)\t0.4162092356742928\n",
      "  :\t:\n",
      "  (16, 13)\t0.3420861038763674\n",
      "  (16, 86)\t0.3420861038763674\n",
      "  (16, 66)\t0.3420861038763674\n",
      "  (16, 43)\t0.3420861038763674\n",
      "  (16, 90)\t0.30069892146698013\n",
      "  (16, 42)\t0.30069892146698013\n",
      "  (17, 57)\t0.35355339059327373\n",
      "  (17, 69)\t0.35355339059327373\n",
      "  (17, 7)\t0.35355339059327373\n",
      "  (17, 65)\t0.35355339059327373\n",
      "  (17, 101)\t0.35355339059327373\n",
      "  (17, 20)\t0.35355339059327373\n",
      "  (17, 30)\t0.35355339059327373\n",
      "  (17, 58)\t0.35355339059327373\n",
      "  (18, 40)\t0.42465483597965126\n",
      "  (18, 104)\t0.42465483597965126\n",
      "  (18, 29)\t0.37327810082857915\n",
      "  (18, 88)\t0.42465483597965126\n",
      "  (18, 79)\t0.42465483597965126\n",
      "  (18, 75)\t0.37327810082857915\n",
      "  (19, 26)\t0.4577405549330346\n",
      "  (19, 47)\t0.4577405549330346\n",
      "  (19, 11)\t0.4577405549330346\n",
      "  (19, 0)\t0.4577405549330346\n",
      "  (19, 29)\t0.4023609542169693\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tf_idf_matrix = tf_idf.fit_transform(youtube_data['Title']);\n",
    "print(tf_idf)\n",
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a6afe",
   "metadata": {},
   "source": [
    "Cosine similarity\n",
    "By applying the definition of similarity, this will be in fact equal to 1 if the two vectors are identical, \n",
    "and it will be 0 if the two are orthogonal. In other words, the similarity is a number bounded between \n",
    "0 and 1 that tells us how much the two vectors are similar. \n",
    "Now that we have numerical vectors, representing each tweets plot description, here i tried can compute similarity of tweets content\n",
    "by calculating their pair-wise cosine similarities\n",
    "and storing them in cosine similarity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e052ba21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.23234803 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.11526946\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         1.         0.         0.         0.11393345 0.12138604\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.23234803 0.         1.         0.         0.11559326 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.12357332 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.11393345 0.11559326 0.         1.         0.07977812\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.12138604 0.         0.         0.07977812 1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.12060364 0.        ]\n",
      " [0.         0.         0.         0.12357332 0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.27867308 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27867308 1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.11001199 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.21207615\n",
      "  0.3073269  0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.11526946 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.21207615 1.\n",
      "  0.16877535 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.3073269  0.16877535\n",
      "  1.         0.         0.         0.         0.08240205 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.33998276 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33998276 0.         1.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11001199 0.         0.\n",
      "  0.08240205 0.         0.         0.         1.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.12060364\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.15019253]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.15019253 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity_matrix = cosine_similarity(tf_idf_matrix, tf_idf_matrix)\n",
    "print(cosine_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4d09acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_from_title(df,title):\n",
    "    return df[df['Title']==title].index.values[0]\n",
    "\n",
    "\n",
    "# function that returns the title of the movie from its index\n",
    "\n",
    "def title_from_index(df,index):\n",
    "\n",
    "    return df[df.index==index].title.values[0]\n",
    "\n",
    "\n",
    "# generating recommendations for given title\n",
    "\n",
    "def recommendations( title, df,cosine_similarity_matrix,number_of_recommendations):\n",
    "\n",
    "    index = index_from_title(df,title)\n",
    "\n",
    "    similarity_scores = list(enumerate(cosine_similarity_matrix[index]))\n",
    "\n",
    "    similarity_scores_sorted = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    recommendations_indices = [t[0] for t in similarity_scores_sorted[1:(number_of_recommendations+1)]]\n",
    "\n",
    "    return df['title'].iloc[recommendations_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81377015",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstock\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myoutube_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcosine_similarity_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mrecommendations\u001b[0;34m(title, df, cosine_similarity_matrix, number_of_recommendations)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecommendations\u001b[39m( title, df,cosine_similarity_matrix,number_of_recommendations):\n\u001b[0;32m---> 16\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mindex_from_title\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     similarity_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(cosine_similarity_matrix[index]))\n\u001b[1;32m     20\u001b[0m     similarity_scores_sorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(similarity_scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mindex_from_title\u001b[0;34m(df, title)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex_from_title\u001b[39m(df,title):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "recommendations('stock', youtube_data, cosine_similarity_matrix, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256933a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea45f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
